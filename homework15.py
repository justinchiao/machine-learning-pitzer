# -*- coding: utf-8 -*-
"""Copy of homework15.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/145KRWSufFjhR_wv841-MZe-HuTip0cW6

**Homework 15.**

In this assignment you will use pre-packaged implementations from Keras (part of Google's TensorFlow) to rebuild the Neural Network from Homework 14. **This is mostly a reading assignment, so study all code below carefully!** Here are the imports you'll need:
"""

import numpy as np

#Helper Functions:
from sklearn.preprocessing import StandardScaler #Same as our Scaler function
from sklearn.model_selection import train_test_split #Same as our TrainTestSplit function
from tensorflow.keras.utils import to_categorical #Same as our OneHot function

#Stochastic Gradient Descent optimizer:
from tensorflow.keras.optimizers import SGD

from tensorflow.keras.optimizers import Adam
#Neural Network building:
from tensorflow.keras.models import Sequential #Same as our Model class
from tensorflow.keras.layers import InputLayer #First layer of any network
from tensorflow.keras.layers import Dense #Same as our Linear class
from tensorflow.keras.layers import ReLU #Same as our ReLU class
from tensorflow.keras.layers import Softmax #Same as our Softmax class

#New layers:
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Flatten

#Import data and target:
from sklearn.datasets import load_digits
X=load_digits().data
y=load_digits().target

print(X)
#Preprocessing

#One Hot encode y:
Y=to_categorical(y)

#Train/test split
Xtrain,Xtest,Ytrain,Ytest=train_test_split(X,Y,train_size=0.8)

#Scale Xtrain and Xtest:
xscaler=StandardScaler()
xscaler.fit(Xtrain)
Xtrain_scaled=xscaler.transform(Xtrain).reshape(1437,8,8,1) #This should look VERY familiar
Xtest_scaled=xscaler.transform(Xtest).reshape(360,8,8,1)

# #Build the network:
# digitsNN=Sequential() #We had digitsNN=Model([]) before
# digitsNN.add(InputLayer(input_shape=(64,))) #New layer type, just to create input.
# digitsNN.add(Dense(32)) #'Dense' takes the place of 'Linear'. Only need to specify output size.
# digitsNN.add(ReLU())
# digitsNN.add(Dense(10))
# digitsNN.add(ReLU())
# digitsNN.add(Dense(10))
# digitsNN.add(Softmax())

"""Run this code block to make sure everything looks OK, and see how many parameters are in your network at each layer."""

# digitsNN.summary()

"""Model training in Keras is separated into two stages:
1. Compilation. We use digitsNN.compile() with stochastic gradient descent (the "optimizer") with a learning rate of 0.01, categorical cross-entropy loss, and accuracy as the metric.
2. Training. We use digitsNN.fit() to train. Using a batch_size of 500 and 5000 epochs would completely reproduce the results of the last assignment. Unfortunately, this takes about 5 minutes to complete (I have no idea why!!). To save some time, we'll just use 100 epochs for now, so you can see the model training in action.
"""

# digitsNN.compile(optimizer=SGD(learning_rate=0.01),loss='categorical_crossentropy',metrics=['accuracy'])

# digitsNN.fit(Xtrain_scaled,Ytrain,batch_size=500,epochs=100)

"""To check the model performance on the test sets, we can use digitsNN.evaluate()."""

# digitsNN.evaluate(Xtest_scaled,Ytest)

"""Now, go back and rebuild the digitsNN network, adding Batch Normalization and Dropout (with a rate of 0.1) after each ReLU layer. This should help prevent over-fitting, and improve accuracy on the test set by almost 2% over what we got in the last assignment. This time, use 5000 epochs like we did homework 14, and be prepared to wait about 5 minutes. (You can speed things up a little by changing the runtime type to GPU.)"""

digitsNN=Sequential() #We had digitsNN=Model([]) before
digitsNN.add(InputLayer(input_shape=(8,8,1))) #New layer type, just to create input.

digitsNN.add(Conv2D(filters=5,kernel_size=(5,5),padding='same',activation='relu'))
digitsNN.add(BatchNormalization())
# digitsNN.add(MaxPooling2D())
digitsNN.add(Conv2D(filters=10,kernel_size=(5,5),padding='same',activation='relu'))
digitsNN.add(BatchNormalization())
# digitsNN.add(MaxPooling2D())
digitsNN.add(Conv2D(filters=20,kernel_size=(5,5),padding='same',activation='relu'))
digitsNN.add(BatchNormalization())
# digitsNN.add(MaxPooling2D())
digitsNN.add(Flatten())
digitsNN.add(Dense(20, activation='relu')) #'Dense' takes the place of 'Linear'. Only need to specify output size.
digitsNN.add(BatchNormalization())
digitsNN.add(Dropout(0.2))
digitsNN.add(Dense(10))
digitsNN.add(Softmax())
digitsNN.compile(optimizer=Adam(learning_rate=0.01),loss='categorical_crossentropy',metrics=['accuracy'])

"""When you are done, run this code block and upload your colab to Gradescope:"""

digitsNN.fit(Xtrain_scaled,Ytrain,batch_size=500,epochs=1000,verbose='auto')
loss,accuracy=digitsNN.evaluate(Xtest_scaled,Ytest)