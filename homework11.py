# -*- coding: utf-8 -*-
"""homework11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PfMZ1hjmLNNkTdXQPG3P4otv4_-DYSAD

**Homework 11**

We'll start with a few imports and our running library of functions.
"""

import pandas as pd
import numpy as np
# import matplotlib.pyplot as plt
from sklearn.datasets import load_digits

def TrainTestSplit(X,y,p,seed=1):
  '''Splits feature matrix X and target array y into train and test sets
  p is the fraction going to train'''
  np.random.seed(seed) #controls randomness
  size=len(y)
  train_size=int(p*size)
  train_mask=np.zeros(size,dtype=bool)
  train_indices=np.random.choice(size, train_size, replace=False)
  train_mask[train_indices]=True
  test_mask=~train_mask
  X_train=X[train_mask]
  X_test=X[test_mask]
  y_train=y[train_mask]
  y_test=y[test_mask]
  return X_train,X_test,y_train,y_test

def PolyFeatures(x,d):
  X=np.zeros((len(x),d+1))
  for i in range(d+1):
    X[:,i]=x**i
  return X

def AddOnes(X):
  return np.concatenate((X,np.ones((len(X),1))),axis=1)

class Scaler:
  def __init__(self,z):
    self.min=np.min(z,axis=0)
    self.max=np.max(z,axis=0)

  def scale(self,x):

    return (x-self.min)/(self.max-self.min+0.000001)

  def unscale(self,x):
    return x*(self.max-self.min+0.000001)+self.min


def LinearSGD(X,y,epochs,batch_size,lr,alpha=0,beta=0):
  '''Stochastic Gradient Descent With L1 and L2 regularization'''
  #alpha=amount of L1 (Lasso) regularization
  #beta=amount of L2 (Ridge) regularization
  X=np.array(X) #Just in case X is a DataFrame
  y=np.array(y) #Just in case y is a Series
  n=len(X)
  coeff=np.ones(X.shape[1]) #Initialize all coeff to be 1 (something to play with?)
  indices=np.arange(len(X))
  for i in range(epochs):
    np.random.seed(i)
    np.random.shuffle(indices)
    X_shuffle=X[indices]
    y_shuffle=y[indices]
    num_batches=n//batch_size
    for j in range(num_batches):
      X_batch=X_shuffle[j*batch_size:(j+1)*batch_size]
      y_batch=y_shuffle[j*batch_size:(j+1)*batch_size]
      resid=X_batch@coeff-y_batch
      gradient=lr*((X_batch.T)@resid)/len(X_batch)+alpha*(2*(coeff>0)-1)+beta*coeff
      coeff=coeff-gradient #Gradient Descent step.
    if n%batch_size!=0: #Check if there is a smaller leftover batch
      X_batch=X_shuffle[num_batches*batch_size:] #last batch
      y_batch=y_shuffle[num_batches*batch_size:] #last batch
      resid=X_batch@coeff-y_batch
      gradient=lr*((X_batch.T)@resid)/len(X_batch)+alpha*(2*(coeff>0)-1)+beta*coeff
      coeff=coeff-gradient
  return coeff

def LinearPredict(X,coeff): #If X was scaled, then this will return scaled predictions
  return X@coeff

def MSE(pred,y):
  return np.sum((pred-y)**2)/len(y)

def sigmoid(t):
  return 1/(1+np.exp(-t))

def LogisticProbability(X,coeff):
  return sigmoid(X@coeff)

def LogisticPredict(X,coeff):
  return X@coeff>0

def accuracy(pred,y):
  return np.sum(pred==y)/len(y)

def LogLoss(probs,y):
  return np.sum(y*np.log(probs)+(1-y)*np.log(1-probs))/len(y)

def LogisticSGD(X,y,epochs,batch_size,lr,alpha=0,beta=0):
  '''Stochastic Gradient Descent for Logistic Regression'''
  #alpha=amount of L1 (Lasso) regularization
  #beta=amount of L2 (Ridge) regularization
  X=np.array(X)
  y=np.array(y)
  n=len(X)
  coeff=np.ones(X.shape[1])
  indices=np.arange(len(X))
  for i in range(epochs):
    np.random.seed(i)
    np.random.shuffle(indices)
    X_shuffle=X[indices]
    y_shuffle=y[indices]
    num_batches=n//batch_size
    for j in range(num_batches):
      X_batch=X_shuffle[j*batch_size:(j+1)*batch_size]
      y_batch=y_shuffle[j*batch_size:(j+1)*batch_size]
      probs=LogisticProbability(X_batch,coeff)
      grad=X_batch.T@(probs-y_batch)/len(X_batch)
      gradient=lr*grad+alpha*(2*(coeff>0)-1)+beta*coeff
      coeff=coeff-gradient
    if n%batch_size!=0: #Check if there is a smaller leftover batch
      X_batch=X_shuffle[num_batches*batch_size:] #last batch
      y_batch=y_shuffle[num_batches*batch_size:] #last batch
      probs=LogisticProbability(X_batch,coeff)
      grad=X_batch.T@(probs-y_batch)/len(X_batch)
      gradient=lr*grad+alpha*(2*(coeff>0)-1)+beta*coeff
      coeff=coeff-gradient
  return coeff

"""Write a function that takes a feature matrix `X` with shape (m,n), and a coefficient matrix `coeff` of shape (n,k), and returns an array with m elements that gives the predicted class of a softmax regression model. Here, m is the number of observations (rows of `X`), n is the number of features (columns of `X`), and k is the number of classes.  The `i`th element of the array that is returned should be the column number of the maximum element of the `i`th row of `X` times `coeff`."""

def SoftmaxPredict(X,coeff):
  return np.argmax(SoftmaxProbability(X,coeff), axis=1)

"""Write a function that takes a feature matrix `X` and a coefficient matrix `coeff` (as in the previous problem), and returns a matrix of predicted probabilities of shape (m,k). The entry in row `i`, column `j`, is the probability that the observation in row `i` of X is in class `j`. This is found by the formula
$$\frac{exp(s^j_i)}{\sum \limits _{j=1} ^k exp(s^j_i)},$$
where $s^j_i$ is the entry in row `i`, column `j`, of the matrix that is the product of `X` and `coeff`. Note that this formula will always be a number between 0 and 1.
"""

def SoftmaxProbability(X,coeff):
  pred=np.exp(X@coeff)
  sums=np.repeat(np.array([pred.sum(axis=1)]).T, pred.shape[1], axis=1)
  return pred/sums
  # return np.exp(X@coeff)/np.repeat(np.array([np.sum(np.exp(X@coeff),axis=1)]).T, np.exp(X@coeff).shape[1], axis=1)

"""Write a function `OneHot` that takes a target array `y` with m entries, where each entry is one of k possible classes, and returns a matrix of shape (m,k), where each entry is a 1 or a 0. If there is a 1 in row `i`, column `j`,  of OneHot(y), that would indicate that the `i`th entry of `y` is the number `j`."""

def OneHot(y):
  return np.array(pd.get_dummies(pd.Series(y)))

OneHot(pd.Series([1,2,3,2,3,1,2,2,3,4]))

"""The function `SoftmaxSGD` finds the coefficients of softmax regression by gradient descent. The code is almost identical to the LogisticSGD function at the top of this colab. Read the code below, paying special attention ot the comments, and complete the two missing lines indicated by "#YOUR CODE HERE".

Note that the gradient of the categorical cross-entropy function (the loss function used in Softmax regression) is given by
$$X^{T}(probs-\overline{y})$$
where $probs$ is given by `SoftmaxProbability(X,coeff)` and $\overline{y}$ is the One Hot enoding of the target vector $y$.
"""

def SoftmaxSGD(X,y,epochs,batch_size,lr,alpha=0,beta=0):
  '''Stochastic Gradient Descent for Softmax Regression'''
  X=np.array(X)
  y=np.array(y)
  classes=np.max(y)+1 #THIS IS NEW!
  ybar=OneHot(y) #THIS IS NEW!
  n=len(X)
  coeff=np.ones((X.shape[1],classes)) #initial coeff matrix. Note the shape!
  indices=np.arange(len(X))
  for i in range(epochs):
    np.random.seed(i)
    np.random.shuffle(indices)
    X_shuffle=X[indices]
    y_shuffle=ybar[indices] #NOTE THE CHANGE FROM y to ybar
    num_batches=n//batch_size
    for j in range(num_batches):
      X_batch=X_shuffle[j*batch_size:(j+1)*batch_size]
      y_batch=y_shuffle[j*batch_size:(j+1)*batch_size]
      probs=SoftmaxProbability(X_batch,coeff)
      grad=X_batch.T@(probs-y_batch)/len(X_batch) #NOTHING NEW HERE!!!!
      gradient=lr*grad+alpha*(2*(coeff>0)-1)+beta*coeff
      coeff=coeff-gradient
    if n%batch_size!=0: #Check if there is a smaller leftover batch
      X_batch=X_shuffle[num_batches*batch_size:]
      y_batch=y_shuffle[num_batches*batch_size:]
      probs=SoftmaxProbability(X_batch,coeff)
      grad=X_batch.T@(probs-y_batch)/len(X_batch)
      gradient=lr*grad+alpha*(2*(coeff>0)-1)+beta*coeff
      coeff=coeff-gradient
  return coeff

"""We now load the `digits` feature matrix and target that we first encountered in Homework 4."""

X=load_digits().data
y=load_digits().target

"""As usual, do the following:
1. 80/20 Test-train split to obtain Xtrain, Xtest, ytrain and ytest
2. Scale Xtrain and Xtest
3. Add a column of ones to Xtrain and Xtest
"""

X_train,X_test,y_train,y_test=TrainTestSplit(X,y,0.8)
xscaler=Scaler(X_train)
yscaler=Scaler(y_train)

X_train_scaled=xscaler.scale(X_train)
X_test_scaled=xscaler.scale(X_test)

X_train_scaled=AddOnes(X_train_scaled)
X_test_scaled=AddOnes(X_test_scaled)

"""Find the coefficient matrix of a softmax regression model to predict which digit is which. Use 10000 epochs, with batch sizes of 5000, and a learning rate of 0.01."""

coeff=SoftmaxSGD(X_train_scaled,y_train,10000,5000,0.01)
coeff

"""Generate predictions for your model on the test set Xtest."""

pred=SoftmaxPredict(X_test_scaled,coeff)
pred

"""Evaluate the accuracy of those predictions. (You may use the `accuracy` function at the top of this colab.)"""

acc=accuracy(pred,y_test)
acc