# -*- coding: utf-8 -*-
"""homework18.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uW57ge1y4S6dQgq0fSE46DG_7mfT6Cnc

**Homework 18**

In this assignment your will train a RNN to predict characters of *Alice in Wonderland*, from strings of consecutive characters.

We begin as usual with the imports you will need for this assignment.
"""

import numpy as np
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import InputLayer
from tensorflow.keras.layers import Softmax
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dropout

from tensorflow.keras.layers import LSTM

"""Run the following text block to read *Alice in Wonderland* from the web, store it in the variable `text`, convert to lower case and remove punctuation."""

import string
from urllib.request import urlopen
url='https://gist.githubusercontent.com/phillipj/4944029/raw/75ba2243dd5ec2875f629bf5d79f6c1e4b5a8b46/alice_in_wonderland.txt'
text = urlopen(url).read().decode('utf-8')
text=text.lower()
text=[c for c in text if (c not in string.punctuation) and (c!='\n')]

"""Write a class `Tokenizer` with the following methods:


*   `__init__`, a method that builds a dictionary `tokens` whose keys are the set of unique characters in some input `text`, and values are integers.
*   `encode`, a method that takes in a corpus of text, converts each character according to the dictionary built by the __init__ method, and outputs a list of those integers.
*   `decode`, a method that takes a single integer (a value from the dictionary), and returns the corresponding character key.


"""

class Tokenizer():
  def __init__(self,text):
    #YOUR CODE HERE
    tokens={}
    reverse_tokens={}
    counter=0
    for i in text:
      if i in tokens:
        pass
      else:
        tokens[i]=counter
        reverse_tokens[counter]=i
        counter+=1
    self.tokens=tokens
    self.reverse_tokens=reverse_tokens


  def encode(self,text):
    self.encoded=list(map(lambda x: self.tokens[x], text))
    return self.encoded

  def decode(self,n):
    return self.reverse_tokens[n]

"""Now, create an object called `tok` of your `Tokenizer` class, and use it to encode `text` as a list of integers, `text_indices`."""

tok=Tokenizer(text)
text_indices=tok.encode(text)

"""For convenience, we'll define `n` to be the length of your tokenizer dictionary:"""

n=len(tok.tokens)

n

"""The next task is to create feature sequences and targets. From `text_indices`, create a list-of-lists `X`. Each sublist of `X` should correspond to 50 consecutive elements of `text_indices`. At the same time, create a list `y` which contains the indices of the characters that follow each sublist of `X`. For example, `X[0]` should be a list containing the first 50 elements of `text_indices`: `text_indices[0]` through `text_indices[49]`. `y[0]` should be the 51st element, `text_indices[50]`. Something very similar was done in Homework 17.

To keep the size of the feature and target vectors manageable, consecutive lists in `X` should be shifted by 3, so the overlap is 47 elements. Hence, `X[1]` should be a list containing the integers `text_indices[3]` through `text_indices[52]`, and `y[1]` should be the integer `text_indices[53]`.
"""

X=[]
y=[]
seq_len=50
for i in range(0,len(text_indices)-seq_len-1,3):
  X.append(text_indices[i:i+seq_len])
  y.append(text_indices[i+seq_len])

"""Convert `X` and `y` to numpy arrays with the same names, and check their shapes. If done correctly, the shape of `X` should be (45539, 50) and the shape of `y` should be (45539, ):"""

X=np.array(X)
y=np.array(y)
X.shape, y.shape

"""Use the `to_categorical` function again to convert both `X` and `y` to one-hot encoded vectors of 0's and 1's, and check their shapes again. You should now have shapes (45539,50,29) and (45539,29). In other words, the vector `X` now contains 45,539 sequences of length 50, and each element of each sequence is a 29-dimensional vector of 28 zeros and a single one in the entry corresponding to some character in the text."""

X=to_categorical(X)
y=to_categorical(y)
X.shape, y.shape

"""You're now ready to create your model. Create a neural network called `model`. This should have an input layer, a recurrent layer with 128 neurons, a dense layer, and a softmax layer. For your recurrent layer, you can use SimpleRNN, or something more sophsticated like an LSTM. (You'll get better results with  LSTM, but it will take MUCH longer. You can mitigate this by reducing the length of each sequence in X down to 10.) The number of neurons in your dense layer should be appropriate to predict the categorical variable `y`."""

model=Sequential()
model.add(InputLayer((50,29)))
model.add(LSTM(128))
model.add(Dense(60,activation='relu'))
model.add(Dense(29,activation='relu'))
model.add(Softmax())

model.summary()

"""Compile your model using the `Adam` optimizer and an approporiately chosen loss function."""

model.compile(optimizer=Adam(learning_rate=0.01),loss='categorical_crossentropy')

"""Fit your data to X and y. Train for 50 epochs with a batch size of 128. Each epoch will take about 95 seconds, so you'll want to leave your computer for about an hour for this to complete."""

model.fit(X,y,batch_size=128,epochs=200)

"""We will now use your trained model to generate text, one character at a time. Run the following code block to do this. (It will take a minute or two to complete.) Its interesting that although the model generates one character at a time, you'll see very word-like strings in the final text."""

seq_len=50
seq=[np.random.randint(0,n) for i in range(seq_len)] #50 random integers for inital prediction
seq=to_categorical(np.array(seq),num_classes=n) #one-hot encode initial sequence

newtext=''
for i in range(500):
  pred_probs=model.predict(seq.reshape(1,seq_len,n)) #Use model to generate probs for next char
  index_pred=np.random.choice(n,1,p=pred_probs.reshape(n))[0] #choose one
  newtext+=tok.decode(index_pred) #corresponding character
  seq=np.vstack([seq,to_categorical(index_pred,num_classes=n)]) #add element to end of sequence
  seq=seq[1:] #remove 1st element from sequence so we have another sequence of length 50

print(newtext) #display generated text

"""**COPY AND PASTE THIS TEXT INTO THE SUBMISSION WINDOW ON GRADESCOPE**"""