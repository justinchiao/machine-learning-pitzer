# -*- coding: utf-8 -*-
"""homework16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PcvbdjxJP7c1VELYLtTC4hpuePH02bjN

**Hommework 16**

In this assignment you will create a Convolutional Neural Network to do facial recognition. We begin by importing a dataset of face photos. This is a very large dataset, and may take a while to load. Once it is complete we will just look at a subset, consisting of people for whom there are at least 70 photos.
"""

import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import fetch_lfw_people
lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)
faces=lfw_people.images
names=lfw_people.target_names
target=lfw_people.target

# for name in names:
#   print(name)

faces.shape

"""You see there are a total of 1288 images, each is 50-by-37 pixels, representing seven people. Let's take a look at one:"""

plt.imshow(faces[10],cmap='gray')

# target[10]

"""You see the target for image 10 is the number 3. Notice that President George W. Bush (the man in the photo) is the 3rd name on the list above (counting from 0). For convenience, we will name this image `bush`:"""

bush=faces[10]

def Conv(image,kernel):
  conv=[]
  kern_h,kern_w=kernel.shape
  for i in range(image.shape[0]-kern_h+1):
    row=image[i].tolist()
    new_row=[]
    for j in range(len(row)-kern_w+1):
      new_row=new_row+[np.sum(image[i:i+kern_h,j:j+kern_w]*kernel)]
    conv=conv+[new_row]
  return np.array(conv)

"""To see the effect of your code, we define a 7-by-7 kernel:"""

kernel=np.zeros((7,7))
kernel[3,:]=1
# print(kernel)

"""We now apply this filter to the image of George Bush."""

Conv(bush,kernel).shape

plt.imshow(Conv(bush,kernel),cmap='gray')

"""You can see this kernel has the effect of horizontally smearing the image.

The next element of a CNN is a way to downsample the image to something of lower resolution. Implement a `MaxPool` function which takes an image and a tuple called "pool_size". If the pool_size is (n,m), then the function should output a lower resolution image where each n-by-m window of the original is replaced by a single pixel whose intensity is the maximum value in the window.
"""

def MaxPool(image,pool_size):
  pooled=[]
  stride=2
  row_index=list(range(0,image.shape[0]-pool_size[0],stride))
  if row_index[-1]+pool_size[0]>image.shape[0]:
    row_index=row_index[0:-1]

  col_index=list(range(0,image.shape[1]-pool_size[1],stride))
  if col_index[-1]+pool_size[1]>image.shape[1]:
    col_index=col_index[0:-1]

  for i in row_index:
    row=image[i].tolist()
    new_row=[]
    for j in col_index:
      new_row=new_row+[np.max(image[i:i+pool_size[0],j:j+pool_size[1]])]
    pooled=pooled+[new_row]
  return np.array(pooled)
  #YOUR CODE HERE

"""We can see the effect of this by applying it to the smeared image of Bush:"""

plt.imshow(MaxPool(Conv(bush,kernel),(2,2)),cmap='gray')

"""Most of the features are now gone, but the basic mouth shape is still there. Hence, this particular kernel, followed by a MaxPooling, may be good at picking out mouth shapes. A different kernel might be useful for picking out eye shapes, nose shapes, etc. In the remainder of this assignment, you'll use pre-packaged Keras implementations to write a CNN which will decide for itself which kernels are most useful for facial recognition.

Here are the imports that you will need:
"""

#Helper Functions:
from sklearn.preprocessing import StandardScaler #Same as our Scaler function
from sklearn.model_selection import train_test_split #Same as our TrainTestSplit function
from tensorflow.keras.utils import to_categorical #Same as our OneHot function

#Optimizers:
from tensorflow.keras.optimizers import SGD #Stochastic Gradient Descent
from tensorflow.keras.optimizers import Adam #Variation of SGD where learning rate decreases

#Neural Network building:
from tensorflow.keras.models import Sequential #Same as our Model class
from tensorflow.keras.layers import InputLayer #First layer of any network
from tensorflow.keras.layers import Dense #Same as our Linear class
from tensorflow.keras.layers import ReLU #Same as our ReLU class
from tensorflow.keras.layers import Softmax #Same as our Softmax class
from tensorflow.keras.layers import Dropout #From Homework 15
from tensorflow.keras.layers import BatchNormalization #From Homework 15

#New layers:
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Flatten #Use this to transition from 2D arrays to 1D before Dense layers

"""Do an 80/20 train/test split:"""

faces_train, faces_test, target_train, target_test=train_test_split(faces,target,train_size=0.8)
faces_train.shape

"""The faces dataset is already scaled appropriately, so we can skip that step. Next, convert the training and testing targets to one-hot encodings."""

y_train=to_categorical(target_train)
y_test=to_categorical(target_test)

"""Create a CNN!"""

model=Sequential()
model.add(InputLayer(input_shape=(50,37,1)))
model.add(Conv2D(filters=10,kernel_size=(5,5),padding='same',activation='relu'))
model.add(BatchNormalization())
# model.add(MaxPooling2D())
model.add(Conv2D(filters=20,kernel_size=(5,5),padding='same',activation='relu'))
model.add(BatchNormalization())
# model.add(MaxPooling2D())
model.add(Conv2D(filters=40,kernel_size=(5,5),padding='same',activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D())
model.add(Conv2D(filters=80,kernel_size=(5,5),padding='same',activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D())
model.add(Flatten())
model.add(Dense(80,activation='relu'))
model.add(Dropout(.1))
model.add(Dense(40,activation='relu'))
model.add(Dropout(.1))
model.add(Dense(20,activation='relu'))
model.add(Dropout(.1))
model.add(Dense(7,activation='relu'))
model.add(Softmax())

"""Take a look at your model:"""

# model.summary()

"""Compile your model with categorical crossentropy loss, reporting the accuracy as you train. Try using the Adam optimizer (similar to SGD, but with a decreasing learning rate). The default initial learning rate for this optimizer is 0.001. That's a good place to start, but it's also something you can play with."""

model.compile(optimizer=Adam(learning_rate=0.01),loss='categorical_crossentropy',metrics=['accuracy'])

"""Fit your model to faces_train and y_train.

*Notes.* If you don't specify a batch_size it will default to 32. That's not a bad place to start. Use something like 50-100 epochs to start, and add more if you think you need to.
"""

model.fit(faces_train,y_train,batch_size=50,epochs=100, verbose=0)

"""Finally, evaluate your model on the test set. You should keep tweaking your model until you consistently achieve at least 90% accuracy. (Each time you change your model, you'll need to recompile and refit.)"""

loss,accuracy=model.evaluate(faces_test,y_test)